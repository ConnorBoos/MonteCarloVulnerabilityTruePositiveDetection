library(jsonlite)
library(dplyr)
library(purrr)
library(tidyr)
library(lubridate)
library(anytime)
library(corrplot)

#Pre Processing the Data to be able to use it more effectively in the data frame
vul_data <- bquxjob_5fbc9bba_187d26e7b11 %>%
  mutate(cve = map(cve, ~ fromJSON(.) %>% as.data.frame())) %>%
  unnest(cve)
vul_data_v1 <- vul_data %>%
  mutate(app = map(app, ~ fromJSON(.) %>% as.data.frame())) %>%
  unnest(app)
vul_data_v2 <- vul_data_v1 %>%
  mutate(behaviors = map(behaviors, ~ fromJSON(.) %>% as.data.frame())) %>%
  unnest(behaviors)
 
#Converting Dates to use time to close
vul_data_v2$created_timestamp <- as.Date(as.POSIXct(vul_data_v2$created_timestamp, origin="1970-01-01"))
vul_data_v2$closed_timestamp <- anydate(vul_data_v2$closed_timestamp)
vul_data_v2$last_behavior <- as.Date(as.POSIXct(vul_data_v2$last_behavior, origin="1970-01-01"))
vul_data_v2$updated_timestamp <- as.Date(as.POSIXct(vul_data_v2$updated_timestamp, origin="1970-01-01"))

#Converting Chars to Values
vul_data_v2$cve.base_score <- as.double(vul_data_v2$cve.base_score)
vul_data_v2$cve.exploit_status <- as.integer(vul_data_v2$cve.exploit_status)
vul_data_v2$behaviors.severity <- as.integer(vul_data_v2$behaviors.severity)
vul_data_v2$behaviors.confidence <- as.integer(vul_data_v2$behaviors.confidence)
vul_data_v2$behaviors.severity <- as.integer(vul_data_v2$behaviors.severity)


#Converting Categorical Data into numeric for purposes of the monte carlo

cve_severity <- unclass(factor(vul_data_v2$cve.severity))
vul_data_v3 <- cbind(vul_data_v2, cve_severity)

status_num <- unclass(factor(vul_data_v2$status))
vul_data_v4 <- cbind(vul_data_v3, status_num)

time_diff <- difftime(vul_data_v4$closed_timestamp,vul_data_v4$created_timestamp)
vul_data_v5 <- cbind(vul_data_v4, time_diff)



counts <- table(vul_data_v5$cve.severity)
barplot(counts,main="Severity Distribution", xlab="Severity Level")

#Discussion: This allows us to see the distribution of the issues from this subset of data that a large portion of the issues are of High and Medium Severity

counts1 <- table(vul_data_v5$BU)
barplot(counts1,main="Business Unit Distribution", xlab="BU")

#Discussion: We can see from this subset of data that it all applies to one specific businesss Unit: Storyful so all of these simulations will be relative to only one specific business unit

counts2 <- table(vul_data_v5$status)
barplot(counts2,main="Status Distribution", xlab="Status")

#Looking Distributions

hist(vul_data_v5$cve.base_score,breaks = 8,xlab="CVE Score",main="Histogram CVE Score")
hist(vul_data_v5$max_confidence,breaks = 8,xlab="Confidence",main="Histogram Confidence")
hist(vul_data_v5$behaviors.severity,xlab="Behavior Severity",main="Histogram Behavior Severity")
hist(vul_data_v5$behaviors.confidence,xlab="Behavior Confidence",main="Histogram Behavior Confidence")
hist(vul_data_v5$cve.exploit_status,xlab="Exploit Status",main="Histogram Exploit Status")
hist(as.integer(vul_data_v5$time_diff),xlab="Time Difference",main="Histogram Time Difference")


#To simulate vulnerability conversion, single factor model is used, where the vulnerability risk is determined

" Risk = likelihood * impact

likelihood function = (basescore*.60)+(behavior.severity*.30)+(maxconfidence*.10)

impact_function = (cve_severity*.90)+(exploit_status*.10)"

tpos <- vul_data_v5[vul_data$status=="true_positive",]
hist(tpos$cve_severity)
hist(as.integer(tpos$time_diff))
hist(tpos$behaviors.severity)
hist(tpos$behaviors.confidence)
hist(tpos$cve.base_score)

vul_data_v5$ioc_weight <- NA
vul_data_v5$ioc_weight[vul_data_v5$behaviors.ioc_source=="file_write"] <- 1
vul_data_v5$ioc_weight[vul_data_v5$behaviors.ioc_source==""] <- 0

vul_data_v5$file_weight <- NA
vul_data_v5$file_weight[vul_data_v5$behaviors.filename=="uTorrent.exe" | vul_data_v5$behaviors.filename=="python3.9" ] <- 1
vul_data_v5$file_weight[vul_data_v5$behaviors.filename!="uTorrent.exe" | vul_data_v5$behaviors.filename!="python3.9"] <- 0

hist(vul_data_v5$file_weight) 

#Trying A couple different weights to achieve optimal risk score to encapsulate the true positives

vul_data_v5$likelihood <- (as.double(vul_data_v5$cve.exploit_status)*.33) + (as.integer(as.logical(vul_data_v5$behaviors.pattern_disposition_details$quarantine_file))*.13) + (as.double(vul_data_v5$ioc_weight)*.33) + (as.double(vul_data_v5$behaviors.severity)*.21)
vul_data_v5$impact <- (as.double(vul_data_v5$cve.base_score)*.90) * (as.double((vul_data_v5$cve_severity)^3)*.10) 
vul_data_v5$risk <- as.double(vul_data_v5$likelihood) * as.double(vul_data_v5$impact)

hist(vul_data_v5$risk) 

only_truepos <- vul_data_v5[vul_data_v5$status_num == 5,]
hist(only_truepos$risk) 
only_truepos_ch <- only_truepos[only_truepos$cve.severity == "HIGH" | only_truepos$cve.severity == "CRITICAL" ,]
hist(only_truepos_ch$risk)
hist(vul_data_v5$risk)
hist(vul_data_v5$cve.base_score)
vul_data_v5$cve.exploit_status
only_truepos_ch$cve.exploit_status
only_truepos_ch$behaviors.pattern_disposition_details$quarantine_file
only_truepos_ch$likelihood
only_truepos_ch$impact
only_truepos_ch$ioc_weight
only_truepos_ch$behaviors.severity
hist(only_truepos_ch$cve.base_score)
hist(only_truepos$risk)

nrow(only_truepos_ch[only_truepos_ch$cve.exploit_status > 0,])/nrow(only_truepos_ch)
nrow(only_truepos[only_truepos$cve.exploit_status > 0,])/nrow(only_truepos)

nrow(only_truepos_ch[only_truepos_ch$behaviors.pattern_disposition_details$quarantine_file == "true",])/nrow(only_truepos_ch)
nrow(only_truepos[only_truepos$behaviors.pattern_disposition_details$quarantine_file == "true",])/nrow(only_truepos)

newdf <- only_truepos %>% select(where(is.numeric))
corrplot(cor(newdf))
newdf

#sample_ <- sample(vul_data_v5, size = 1400, replace = F)
sample_test <- sample(vul_data_v5, size = 600, replace = T)

vul_data_v5$impact
vul_data_v5$likelihood
vul_data_v5$risk
vul_data_v5$time_diff

mean(vul_data_v5$risk)
sd(vul_data_v5$risk)
sd(only_truepos$risk)
mean(only_truepos$risk)

hist(log(vul_data_v5$risk))

#Appears more normal perhaps using the log will help with choosing a better method to get accuracy
hist(log(only_truepos_ch$risk))
mean(log(only_truepos_ch$risk))
sd(log(only_truepos$risk))

#Looking at the range interval it would be mean = 3.972 and sd = 0.627
exp(1.254)
exp(1.057)

#Rejection Sampling
curve(dbeta(x, 3,6),0,1)
vul_data_v5$log_trueposrisk <- log(vul_data_v5$risk)
only_truepos_ch$log_trueposrisk <- log(only_truepos_ch$risk)

x <- log(vul_data_v5$risk)
h<-hist(x, breaks=10, col="red", xlab="Log (Risk Score)",
        main="Rejection Sampling")
xfit<-seq(min(x),max(x),length=40)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col="blue", lwd=2)

only_truepos_ch$targetDensity <- dnorm(only_truepos_ch$log_trueposrisk,mean=mean(only_truepos_ch$log_trueposrisk),sd=sd(only_truepos_ch$log_trueposrisk))
vul_data_v5$targetDensity <- dnorm(vul_data_v5$log_trueposrisk,mean=mean(vul_data_v5$log_trueposrisk),sd=sd(vul_data_v5$log_trueposrisk))
vul_data_v5$targetDensity
only_truepos_ch$targetDensity
maxDens = max(only_truepos_ch$targetDensity, na.rm = T)
maxDens
sample_ <- sample(1:nrow(vul_data_v5), 1400, replace = F)
ss <- vul_data_v5[sample_,]
ss$accepted = ifelse(vul_data_v5[sample_,]$targetDensity < ss$targetDensity / maxDens, TRUE, FALSE)
hist(log(ss$risk[ss$accepted]), freq = F, col = "grey", breaks = 10)
lines(xfit, yfit, col="blue", lwd=2)
head(ss$risk[ss$accepted])
#Taking a sample with replacement to see if we can get a closer more accurate distribution to work with for risk scores

runs <- 10000

#Getting True positives for Critical And High
true_positive.track <- function(){
  sample_ <- sample(1:nrow(vul_data_v5), 1400, replace = F)
  ss <- vul_data_v5[sample_,]
  counts_within_risk <- 0
  for(x in 1:length(sample_)){
    if(log(ss$risk[x]) >= 2 & log(ss$risk[x]) <= 4 & ss$cve.base_score[x] >= 8 ){
      counts_within_risk = counts_within_risk + 1
    }
    else{
      counts_within_risk = counts_within_risk
    }
  }

  return(abs(counts_within_risk-nrow(ss[ss$status_num==5,]))/nrow(ss[ss$status_num==5,]))
}

mc.prob <- sum(replicate(runs,true_positive.track()))/runs
mc.prob
#So from here we know that the prime true_positive will have a risk score from 0 to 50 given the cve_base.score is greater than 8. 
#This should allow us to determine to a certain level of accuracy the true positives and help to focus on vulnerabilities that are critical and high.

true_positive1.track <- function(){
  sample_ <- sample(1:nrow(vul_data_v5), 1400, replace = F)
  ss <- vul_data_v5[sample_,]
  ss$accepted = ifelse(vul_data_v5[sample_,]$targetDensity < ss$targetDensity / maxDens, TRUE, FALSE)
  ss <- ss[ss$accepted,]
  counts_within_risk <- 0
  for(x in 1:length(sample_)){
    if(log(ss$risk[x]) >= 2 & log(ss$risk[x]) <= 4 & ss$cve.base_score[x] >= 8 ){
      counts_within_risk = counts_within_risk + 1
    }
    else{
      counts_within_risk = counts_within_risk
    }
  }
  
  return(abs(counts_within_risk-nrow(ss[ss$status_num==5,]))/nrow(ss[ss$status_num==5,]))
}

mc.prob1 <- sum(replicate(runs,true_positive1.track()))/runs
mc.prob1